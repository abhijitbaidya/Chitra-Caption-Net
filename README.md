# Chitra-Caption-Net

Automatic image captioning is a task which combines the challenges of computer vision and natural language processing. It necessitates identifying the key elements, their characteristics, and their connections within an image. One of the most crucial components of image captioning is the concept of attention: how to determine what to describe and how to do it. In this work, we proposed \textbf{Chitra Caption-Net}, which is designed for hybrid tasks involving both spatial and sequential data, incorporating dense embedding and  an implicit decoding transformer consist of custom decoder and encoder. The performance of our proposed model was evaluated on BAN-Cap, BanglaLekha, and Flickr8k (Nepali Version) datasets, and Chitra Caption-Net outperforms other state-of-the-art models, by obtaining BLEU-4, METEOR, and CIDEr scores of 20.86, 26.61, and 21.85; 62.86, 67.16, and 60.57; 69.24, 51.50, and 46.72, respectively.
